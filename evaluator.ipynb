{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a665885b",
   "metadata": {},
   "source": [
    "# Evaluator Module\n",
    "The Evaluator module creates evaluation reports.\n",
    "\n",
    "Reports contain evaluation metrics depending on models specified in the evaluation config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf9140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# reloads modules automatically before entering the execution of code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# third parties imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# -- add new imports here --\n",
    "\n",
    "# local imports\n",
    "from configs import EvalConfig\n",
    "from constants import Constant as C\n",
    "from loaders import export_evaluation_report\n",
    "from loaders import load_ratings\n",
    "from surprise import model_selection, accuracy\n",
    "from models import ModelBaseline1, ModelBaseline2, ModelBaseline3, ModelBaseline4, get_top_n\n",
    "# -- add new imports here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c24a4",
   "metadata": {},
   "source": [
    "# 1. Model validation functions\n",
    "Validation functions are a way to perform crossvalidation on recommender system models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d82188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_split_predictions(algo, ratings_dataset, eval_config):\n",
    "    testsize = eval_config.test_size\n",
    "    trainset, testset = model_selection.train_test_split(ratings_dataset, test_size=testsize)\n",
    "    \"\"\"Generate predictions on a random test set specified in eval_config\"\"\"\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)     \n",
    "    # -- implement the function generate_split_predictions --\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def generate_loo_top_n(algo, ratings_dataset, eval_config):\n",
    "    loo = model_selection.LeaveOneOut(n_splits=1)\n",
    "    n = eval_config.top_n_value\n",
    "    trainset, testset = next(loo.split(ratings_dataset))\n",
    "    anti_testset = trainset.build_anti_testset()\n",
    "\n",
    "    \"\"\"Generate top-n recommendations for each user on a random Leave-one-out split (LOO)\"\"\"\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(anti_testset)\n",
    "    anti_testset_top_n = get_top_n(predictions, n)\n",
    "\n",
    "    # -- implement the function generate_loo_top_n --\n",
    "    return anti_testset_top_n, testset\n",
    "\n",
    "\n",
    "def generate_full_top_n(algo, ratings_dataset, eval_config):\n",
    "    n = eval_config.top_n_value\n",
    "    trainset = ratings_dataset.build_full_trainset()\n",
    "    testset = trainset.build_anti_testset()\n",
    "\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    anti_testset_top_n = get_top_n(predictions, n)\n",
    "\n",
    "    \"\"\"Generate top-n recommendations for each user with full training set (LOO)\"\"\"\n",
    "    # -- implement the function generate_full_top_n --\n",
    "    return anti_testset_top_n\n",
    "\n",
    "\n",
    "def precompute_information():\n",
    "    ratings_dataset = load_ratings(False)\n",
    "    \"\"\" Returns a dictionary that precomputes relevant information for evaluating in full mode\n",
    "    \n",
    "    Dictionary keys:\n",
    "    - precomputed_dict[\"item_to_rank\"] : contains a dictionary mapping movie ids to rankings\n",
    "    - (-- for your project, add other relevant information here -- )\n",
    "    \"\"\"\n",
    "\n",
    "    # Compter le nombre de notes pour chaque item\n",
    "    item_counts = ratings_dataset[C.ITEM_ID_COL].value_counts()\n",
    "\n",
    "    # Utiliser rank pour attribuer un rang basé sur la popularité (1 = le plus populaire)\n",
    "    item_to_rank = item_counts.rank(ascending=False, method=\"min\").to_dict()\n",
    "\n",
    "    precomputed_dict = {}\n",
    "    precomputed_dict[\"item_to_rank\"] = item_to_rank\n",
    "\n",
    "    \n",
    "    return precomputed_dict                \n",
    "\n",
    "\n",
    "def create_evaluation_report(eval_config, sp_ratings, precomputed_dict, available_metrics):\n",
    "    \"\"\" Create a DataFrame evaluating various models on metrics specified in an evaluation config.  \n",
    "    \"\"\"\n",
    "    evaluation_dict = {}\n",
    "    for model_name, model, arguments in eval_config.models:\n",
    "        print(f'Handling model {model_name}')\n",
    "        algo = model(**arguments)\n",
    "        evaluation_dict[model_name] = {}\n",
    "        \n",
    "        # Type 1 : split evaluations\n",
    "        if len(eval_config.split_metrics) > 0:\n",
    "            print('Training split predictions')\n",
    "            predictions = generate_split_predictions(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.split_metrics:\n",
    "                print(f'- computing metric {metric}')\n",
    "                assert metric in available_metrics['split']\n",
    "                evaluation_function, parameters =  available_metrics[\"split\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(predictions, **parameters) \n",
    "\n",
    "        # Type 2 : loo evaluations\n",
    "        if len(eval_config.loo_metrics) > 0:\n",
    "            print('Training loo predictions')\n",
    "            anti_testset_top_n, testset = generate_loo_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.loo_metrics:\n",
    "                assert metric in available_metrics['loo']\n",
    "                evaluation_function, parameters =  available_metrics[\"loo\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(anti_testset_top_n, testset, **parameters)\n",
    "        \n",
    "        # Type 3 : full evaluations\n",
    "        if len(eval_config.full_metrics) > 0:\n",
    "            print('Training full predictions')\n",
    "            anti_testset_top_n = generate_full_top_n(algo, sp_ratings, eval_config)\n",
    "            for metric in eval_config.full_metrics:\n",
    "                assert metric in available_metrics['full']\n",
    "                evaluation_function, parameters =  available_metrics[\"full\"][metric]\n",
    "                evaluation_dict[model_name][metric] = evaluation_function(\n",
    "                    anti_testset_top_n,\n",
    "                    **precomputed_dict,\n",
    "                    **parameters\n",
    "                )\n",
    "        \n",
    "    return pd.DataFrame.from_dict(evaluation_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e83d1d",
   "metadata": {},
   "source": [
    "# 2. Evaluation metrics\n",
    "Implement evaluation metrics for either rating predictions (split metrics) or for top-n recommendations (loo metric, full metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1849e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(anti_testset_top_n, testset):\n",
    "    \"\"\"\n",
    "    Compute the average hit rate over the users (LOO metric).\n",
    "\n",
    "    A hit (1) happens when the movie in the testset has been picked by the top-n recommender.\n",
    "    A fail (0) happens when the movie in the testset has not been picked by the top-n recommender.\n",
    "\n",
    "    Parameters:\n",
    "    - anti_testset_top_n: dict, top-n recommendations for each user (user_id -> list of (movie_id, score))\n",
    "    - testset: list of tuples, each tuple is (user_id, movie_id, rating_value)\n",
    "\n",
    "    Returns:\n",
    "    - hit_rate: float, the proportion of users for whom the testset movie is in the top-n recommendations.\n",
    "    \"\"\"\n",
    "    # testset-> DataFrame\n",
    "    testset_df = pd.DataFrame(testset, columns=[\"user_id\", \"movie_id\", \"rating_value\"])\n",
    "\n",
    "    reco_data = []\n",
    "    # anti_testset_top_n -> DataFrame\n",
    "    for user_id, recommendations in anti_testset_top_n.items():\n",
    "        for movie_id, score in recommendations:\n",
    "            reco_data.append({\"user_id\": user_id, \"movie_id\": movie_id, \"score\": score})\n",
    "\n",
    "    reco_df = pd.DataFrame(reco_data)\n",
    "    \n",
    "    # inner join on the two dataframes to get the hits \n",
    "    merged_df = pd.merge(testset_df, reco_df, on=[\"user_id\", \"movie_id\"], how=\"inner\")\n",
    "\n",
    "    # compute hit rate\n",
    "    hit_rate = len(merged_df) / len(testset_df) if len(testset_df) > 0 else 0\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "def get_novelty(anti_testset_top_n, item_to_rank):\n",
    "    \"\"\"Compute the average novelty of the top-n recommendation over the users (full metric)\n",
    "    \n",
    "    The novelty is defined as the average ranking of the movies recommended\n",
    "\n",
    "    **Limites**\n",
    "    Cette métrique présente des limites en termes de sensibilité à la distribution des données. \n",
    "    En effet, le rang ne capture pas l'écart de popularité entre les films.\n",
    "    \"\"\"\n",
    "    reco_data = []\n",
    "    # anti_testset_top_n -> DataFrame\n",
    "    for user_id, recommendations in anti_testset_top_n.items():\n",
    "        for movie_id, score in recommendations:\n",
    "            reco_data.append({\"user_id\": user_id, \"movie_id\": movie_id, \"score\": score})\n",
    "    \n",
    "    reco_df = pd.DataFrame(reco_data)\n",
    "    # map rankings based on item_to_rank\n",
    "    reco_df[\"rank\"] = reco_df[\"movie_id\"].map(item_to_rank)\n",
    "\n",
    "    # compute average ranking\n",
    "    average_rank_sum = reco_df[\"rank\"].mean() if not reco_df.empty else 0\n",
    "    return average_rank_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9855b3",
   "metadata": {},
   "source": [
    "# 3. Evaluation workflow\n",
    "Load data, evaluate models and save the experimental outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling model baseline_1\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Training full predictions\n",
      "Handling model baseline_2\n",
      "Training split predictions\n",
      "- computing metric mae\n",
      "- computing metric rmse\n",
      "Training loo predictions\n",
      "Training full predictions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AVAILABLE_METRICS = {\n",
    "    \"split\": {\n",
    "        \"mae\": (accuracy.mae, {'verbose': False}),\n",
    "        # -- add new split metrics here --\n",
    "        \"rmse\":(accuracy.rmse, {'verbose': False}),\n",
    "    },\n",
    "    \"loo\" : {\n",
    "        \"hitrate\":(get_hit_rate, {})\n",
    "    },\n",
    "    \"full\" : {\n",
    "        \"novelty\":(get_novelty, {})\n",
    "    }\n",
    "    \n",
    "    # -- add new types of metrics here --\n",
    "}\n",
    "\n",
    "ratingsdata = load_ratings(True)\n",
    "\n",
    "sp_ratings = load_ratings(surprise_format=True)\n",
    "precomputed_dict = precompute_information()\n",
    "evaluation_report = create_evaluation_report(EvalConfig, sp_ratings, precomputed_dict, AVAILABLE_METRICS)\n",
    "display(evaluation_report)\n",
    "\n",
    "export_evaluation_report(evaluation_report)\n",
    "\n",
    "'''\n",
    "Observations :\n",
    "\n",
    "L'algorithme qui présente la novelty la plus basse est le 4 (SVD), \n",
    "qui recommande donc des films plus populaires en moyenne.\n",
    "\n",
    "Elle se démarque également par une erreur moyenne absolue et une erreur quadratique moyenne plus basses que les autres, \n",
    "la rendant plus fiable que les autres algorithmes.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
